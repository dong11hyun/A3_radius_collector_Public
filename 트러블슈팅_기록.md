# 🔧 트러블슈팅 기록: 프로젝트 확장 과정에서 마주한 문제들

> **프로젝트**: Radius Collector - 편의점 폐업 검증 시스템  
> **작성일**: 2026-01-22  
> **문서 목적**: 서울 25개 구 확장 과정에서 발생한 실제 문제와 해결 과정 기록

---

## 📍 1. 다이소 매장 누락 문제 (신길점)

### 🚨 문제 상황
- 다이소 공식 API 호출 시 **신길점이 조회되지 않는 현상** 발생
- 신규 오픈 매장(최근 1개월 이내)이 API에 반영되지 않음

### 🔍 원인 분석
```
API 호출 결과: 23개 매장
실제 네이버/카카오맵 검색: 24개 매장
누락 매장: 다이소 신길점 (2026년 1월 오픈)
```
- **원인**: 다이소 API의 매장 데이터 갱신 주기가 실시간이 아님
- 비공식 API 엔드포인트(`fapi.daisomall.co.kr`)의 데이터 동기화 지연

### ✅ 해결 방법
1. **엔드포인트 체크 추가**: API 응답을 네이버/카카오맵 검색 결과와 비교
2. **교차 검증 도입**: API에서 누락된 매장은 카카오 키워드 검색으로 보완
3. **다이소 데이터 확보 후 편의점 수집 진행**: 기준점(다이소) 완전 확보 후 다음 단계 실행

```python
# 검증 로직 추가
def validate_daiso_completeness(api_results, map_results):
    missing = set(map_results) - set(api_results)
    if missing:
        logging.warning(f"누락 매장: {missing}")
```

---

## 📍 2. 카카오 API 호출 한계 극복 (4분면 분할)

### 🚨 문제 상황
- 카카오맵 API 제한: **15개/페이지 × 최대 3페이지 = 45개**
- 밀집 지역(강남, 명동 등)에서 45개 초과 매장이 존재

### 🔍 원인 분석
```
요청: 반경 2km 내 편의점 검색
결과: 45개 (최대치)
실제: 약 150개 존재 → 105개 누락!
```

### ✅ 해결 방법: Rect(사각형) 4분면 분할

```
기존: 중심점 기준 반경 r 원형 검색
     ┌─────────────────┐
     │        ●        │  ← 45개만 수집
     │     반경 2km    │
     └─────────────────┘

개선: 4개의 1km×1km 사각형으로 분할
     ┌────────┬────────┐
     │   Q2   │   Q1   │  ← 각 45개씩
     │  ●     │     ●  │  = 180개 수집 가능
     ├────────┼────────┤
     │   Q3   │   Q4   │
     │  ●     │     ●  │
     └────────┴────────┘
```

```python
def create_quadrants(center_lat, center_lng, radius_km=1.0):
    delta_lat = 0.0090 * radius_km  # 위도 1km ≈ 0.009
    delta_lng = 0.0113 * radius_km  # 경도 1km ≈ 0.011 (서울 기준)
    
    return [
        (center_lat + delta_lat/2, center_lng + delta_lng/2),  # Q1: 북동
        (center_lat + delta_lat/2, center_lng - delta_lng/2),  # Q2: 북서
        (center_lat - delta_lat/2, center_lng - delta_lng/2),  # Q3: 남서
        (center_lat - delta_lat/2, center_lng + delta_lng/2),  # Q4: 남동
    ]
```

**결과**: 기존 45개 한계 → 4분면 × 45개 = 최대 180개 수집 가능

---

## 📍 3. 공공데이터 정합성 문제 (Sheet vs OpenAPI)

### 🚨 문제 상황
- `소상공인_상권분석.csv` 사용 시 현재 영업 중인 편의점과 불일치
- 실제 폐업한 매장이 "영업중"으로 표시되는 오류 다수 발생

### 🔍 원인 분석

| 데이터 형태 | 특징 | 갱신 주기 |
|-------------|------|----------|
| **Sheet (CSV)** | 정적 데이터, 다운로드 시점 기준 | 월/분기 단위 |
| **OpenAPI** | 동적 데이터, 실시간 서버 조회 | 일/주 단위 |

- 기존 사용 데이터: 소상공인 상권분석 CSV (정적, 업데이트 지연)
- 해당 데이터셋은 OpenAPI 미제공

### ✅ 해결 방법
1. **추가 공공데이터 확보**: OpenAPI 제공 데이터셋 발굴
   - 영등포구 휴게음식점 인허가 데이터 (OpenAPI)
   - 담배소매업 인허가 데이터 (OpenAPI)
   
2. **구별 차등 검증 체계 구축**

   | 구 | 검증 방식 | 데이터소스 |
   |----|----------|-----------|
   | 영등포구 | 3중 교차 검증 | 휴게음식점 + 담배소매업 + CSV |
   | 그 외 24개 구 | 2중 교차 검증 | 휴게음식점 + 담배소매업 |

   ```
   # 영등포구
   검증 = 카카오맵 ∩ (휴게음식점 ∪ 담배소매업 ∪ CSV)
   
   # 그 외 구
   검증 = 카카오맵 ∩ (휴게음식점 ∪ 담배소매업)
   ```

```python
# 구별 검증 로직
if target_gu == '영등포구':
    # 3개 데이터소스 통합 조회 (CSV 포함)
    all_names = restaurant_names | tobacco_names | csv_names
    all_coords = restaurant_coords | tobacco_coords | csv_coords
else:
    # 2개 데이터소스 통합 조회 (CSV 미포함)
    all_names = restaurant_names | tobacco_names
    all_coords = restaurant_coords | tobacco_coords
```

> **참고**: `public_data.csv`는 영등포구 소상공인 상권분석 데이터만 포함되어 있어, 다른 구에서는 사용하지 않음. 향후 전국 확장 시 구별 CSV 확보 또는 전국 OpenAPI 도입 필요.

---

## 📍 4. 좌표계 변환 문제 (TM → WGS84)

### 🚨 문제 상황
- 카카오 API: WGS84 좌표계 (위도/경도)
- 공공데이터 OpenAPI: 중부원점 TM (EPSG:5174)
- 두 좌표 직접 비교 불가

### 🔍 원인 분석
```
카카오맵:    (37.52412, 126.92356)  ← WGS84
공공데이터:  (197852.3, 449821.7)   ← EPSG:5174 (미터 단위)
```

### ✅ 해결 방법: pyproj 라이브러리 사용

```python
from pyproj import Proj, transform

# EPSG:5174 (중부원점TM) → EPSG:4326 (WGS84) 변환
proj_tm = Proj(init='epsg:5174')
proj_wgs = Proj(init='epsg:4326')

def tm_to_wgs(x, y):
    lng, lat = transform(proj_tm, proj_wgs, x, y)
    return round(lat, 4), round(lng, 4)
```

**좌표 정밀도 기준 설정**
| 소수점 자릿수 | 오차 범위 | 용도 |
|--------------|----------|------|
| 3자리 | ~70m | 지역 구분 |
| **4자리** | **~7m** | **매장 동일성 판별** ✅ |
| 5자리 | ~0.7m | 정밀 측위 |

---

## 📍 5. 데이터 중복 제거

### 🚨 문제 상황
- 4분면 경계에서 동일 매장이 중복 수집
- 여러 데이터소스에서 같은 매장이 다른 이름으로 등록

### 🔍 원인 분석
```
Q1에서 수집: "GS25 신길역점" (37.5124, 126.9178)
Q3에서 수집: "GS25 신길역점" (37.5124, 126.9178)  ← 중복!
```

### ✅ 해결 방법

```python
def deduplicate_stores(stores):
    seen_coords = set()
    unique_stores = []
    
    for store in stores:
        coord_key = (round(store['lat'], 4), round(store['lng'], 4))
        if coord_key not in seen_coords:
            seen_coords.add(coord_key)
            unique_stores.append(store)
    
    return unique_stores
```

**적용 결과**
- 수집 데이터: 1,247개
- 중복 제거 후: 1,089개 (12.7% 중복)

---

## 📍 6. 최적 반경 테스트 (구별 경계선 검증)

### 🚨 문제 상황
- 일률적인 반경(2km) 적용 시 일부 구에서 커버리지 부족
- 구마다 면적과 다이소 분포가 달라 최적 반경이 다름

### 🔍 원인 분석
| 구 | 면적 (km²) | 다이소 수 | 권장 반경 |
|----|-----------|----------|----------|
| 중구 | 9.96 | 8개 | 1.0km |
| 강남구 | 39.5 | 15개 | 1.5km |
| 강서구 | 41.4 | 12개 | 2.0km |

### ✅ 해결 방법: 커버리지 테스트 자동화

```python
# test_core.py
def test_optimal_radius_by_gu():
    """각 구별 최소 커버리지 70% 달성 반경 계산"""
    for gu, boundary in SEOUL_GU_BOUNDARIES.items():
        for radius in [0.5, 0.8, 1.0, 1.3, 1.5, 2.0]:
            coverage = calculate_coverage(boundary, daiso_locations, radius)
            if coverage >= 0.70:
                optimal_radii[gu] = radius
                break
```

**테스트 결과 요약**
```
최소 반경 1.0km: 8개 구 (중구, 종로구, 용산구...)
최소 반경 1.3km: 12개 구 (영등포구, 마포구, 성동구...)
최소 반경 1.5km: 5개 구 (강남구, 서초구, 송파구...)
```

---

## 📍 7. Airflow 도입 적합성 검토

### 🚨 고민했던 점
- 5단계 순차 파이프라인을 Airflow DAG로 관리할 필요가 있는가?
- 현재 규모에서 Airflow 도입이 오버엔지니어링인가?

### 🔍 분석

| 기준 | 현재 상태 | Airflow 필요성 |
|------|----------|---------------|
| 태스크 수 | 5개 | 낮음 |
| 실행 주기 | 수동/일회성 | 낮음 |
| 의존성 복잡도 | 단순 순차 | 낮음 |
| 팀 규모 | 1인 | 낮음 |
| 스케줄링 요구 | 없음 | 낮음 |

### ✅ 결론: 현재 단계에서는 미도입

```python
# 현재 구조로 충분
# run_all.py로 단순 순차 실행
call_command('v2_3_1_collect_yeongdeungpo_daiso', gu=target_gu)
call_command('v2_3_2_collect_Convenience_Only', gu=target_gu)
call_command('openapi_1', gu=target_gu)
call_command('openapi_2', gu=target_gu)
call_command('check_store_closure', gu=target_gu)
```

**향후 Airflow 도입 시점**
- 25개 구 자동 스케줄링 필요 시
- 병렬 수집 + 알림 시스템 도입 시
- 팀 협업으로 워크플로우 가시화 필요 시

---

## 📍 8. radius_km 파라미터 최적화 (Trade-off)

### 🚨 문제 상황
반경 설정에 따른 Trade-off 존재

| 반경 | 수집 속도 | 정확성 | API 호출 수 |
|------|----------|--------|------------|
| 2.0km | 느림 (50초) | 높음 (누락↓)↑| 적정 |
| 0.5km | 빠름 (20초) | 많이 낮음 (누락↑) | 낮음 |

### 🔍 원인 분석
```
반경 2.0km: 다이소 1개당 4분면 호출, 넓은 범위 탐색으로 정확도 높음 (50초)
반경 0.5km: 다이소 1개당 1회 호출, 좁은 범위로 빠르지만 누락 가능 (20초)
```

### ✅ 해결 방법: 1.8km 반경 선택 (속도-정확성 균형점)

직접 서울 25개 '구'에 대한 (서울시 행정구역 GeoJSON)좌표 추가, **1.8km 반경 + 4분면 분할**이 최적의 균형점으로 선택됨:

```python
# 현재 적용된 설정
RADIUS_KM = 1.8  # 기본 반경

# 4분면 분할로 실제 커버리지 확장
# 1.8km × 4분면 = 약 2.6km 범위 커버
```

| 설정 | 1구 수집 시간 | 커버리지 | 선택 |
|------|-------------|----------|------|
| 0.5km | 20초 | ~70% | ❌ 누락 다수 |
| **1.8km** | **40초** | **~90%** | **✅ 채택** |
| 2.0km | 50초 | ~95% | ❌ 과잉 |

---

## 🔧 백엔드 관련 추가 트러블슈팅  아직 반영은 안시켰음 고민.

### 📍 9. Rate Limit 초과 문제

### 🚨 문제 상황
- 카카오 API 호출 중 `429 Too Many Requests` 에러 발생
- 연속 호출 시 일시적 차단

### ✅ 해결 방법
```python
# 지수 백오프 재시도 로직
import time

def api_call_with_retry(url, max_retries=3):
    for attempt in range(max_retries):
        response = requests.get(url)
        if response.status_code == 429:
            wait_time = 2 ** attempt  # 1, 2, 4초
            time.sleep(wait_time)
            continue
        return response
    raise Exception("Max retries exceeded")

# 호출 간격 추가
time.sleep(0.2)  # 200ms 딜레이
```

---

### 📍 10. Django ORM N+1 쿼리 문제

### 🚨 문제 상황
- 폐업 결과 조회 시 응답 지연 (3초 이상)
- 로그 확인 결과 DB 쿼리 수백 개 발생

### 🔍 원인 분석
```python
# 문제 코드
for result in StoreClosureResult.objects.all():
    print(result.gu)  # 매번 DB 조회
```

### ✅ 해결 방법
```python
# values() 사용으로 필요한 필드만 조회
results = StoreClosureResult.objects.values('name', 'address', 'lat', 'lng', 'gu')

# 또는 select_related 사용 (FK 관계 시)
results = StoreClosureResult.objects.select_related('related_model')
```

**결과**: 쿼리 수 N+1 → 1개, 응답 시간 3초 → 0.1초

---

### 📍 11. 동시성 문제 (Race Condition)

### 🚨 문제 상황
- 두 개의 수집 요청이 동시에 들어왔을 때 데이터 충돌
- `update_or_create` 중 `IntegrityError` 발생

### 🔍 원인 분석
```
Thread A: SELECT → 없음 → INSERT
Thread B: SELECT → 없음 → INSERT  ← 동시 실행 시 충돌!
```

### ✅ 해결 방법
```python
from django.db import transaction

# 트랜잭션 락 사용
with transaction.atomic():
    store, created = YeongdeungpoConvenience.objects.select_for_update().update_or_create(
        place_id=place_id,
        defaults={...}
    )
```

---

### 📍 12. 메모리 누수 (대용량 데이터 처리)

### 🚨 문제 상황
- 25개 구 연속 수집 시 메모리 사용량 지속 증가
- 최종적으로 `MemoryError` 발생

### 🔍 원인 분석
```python
# 문제 코드: 전체 데이터를 리스트로 메모리에 적재
all_stores = list(Store.objects.all())  # 10만 건 전체 로드
```

### ✅ 해결 방법
```python
# iterator() 사용으로 청크 단위 처리
for store in Store.objects.iterator(chunk_size=1000):
    process(store)

# 구 단위 처리 후 메모리 해제
import gc
gc.collect()
```

---

### 📍 13. 타임존 처리 오류

### 🚨 문제 상황
- `collected_at` 필드의 시간이 9시간 차이 발생
- 로그와 DB 기록 불일치

### 🔍 원인 분석
```python
# settings.py
TIME_ZONE = 'UTC'  # 기본값
USE_TZ = True
```

### ✅ 해결 방법
```python
# settings.py 수정
TIME_ZONE = 'Asia/Seoul'
USE_TZ = True

# 코드에서 timezone aware datetime 사용
from django.utils import timezone
collected_at = timezone.now()  # KST 기준
```

---

### 📍 14. CSS/JS 정적 파일 미로드 (Docker 환경)

### 🚨 문제 상황
- 로컬에서는 정상 동작하나 Docker 컨테이너에서 스타일 깨짐
- 콘솔에 `404 Not Found: /static/...` 에러

### 🔍 원인 분석
```python
# DEBUG=True일 때만 Django가 정적 파일 서빙
# Docker에서 실수로 DEBUG=False 설정
```

### ✅ 해결 방법
```python
# settings.py
STATIC_URL = '/static/'
STATIC_ROOT = BASE_DIR / 'staticfiles'

# collectstatic 실행
# python manage.py collectstatic

# 또는 whitenoise 미들웨어 사용
MIDDLEWARE = [
    'whitenoise.middleware.WhiteNoiseMiddleware',
    ...
]
```

---

## 📊 트러블슈팅 요약

| # | 문제 | 원인 | 해결책 | 난이도 |
|---|------|------|--------|--------|
| 1 | 다이소 누락 | API 동기화 지연 | 교차 검증 | ⭐⭐ |
| 2 | API 45개 한계 | 페이징 제한 | 4분면 분할 | ⭐⭐⭐ |
| 3 | CSV 정합성 | 정적 데이터 | OpenAPI 전환 | ⭐⭐ |
| 4 | 좌표계 불일치 | TM vs WGS84 | pyproj 변환 | ⭐⭐ |
| 5 | 데이터 중복 | 분할 경계 중복 | 좌표 기반 dedupe | ⭐ |
| 6 | 반경 최적화 | 구별 밀집도 상이 | 커버리지 테스트 | ⭐⭐⭐ |
| 7 | Airflow 도입 | 오버엔지니어링 | 현재 미도입 | ⭐ |
| 8 | Trade-off | 속도 vs 정확도 | 동적 반경 | ⭐⭐ |
| 9 | Rate Limit | 연속 호출 | 지수 백오프 | ⭐⭐ |
| 10 | N+1 쿼리 | ORM 비효율 | values/select_related | ⭐⭐ |
| 11 | Race Condition | 동시 접근 | select_for_update | ⭐⭐⭐ |
| 12 | 메모리 누수 | 대용량 로드 | iterator + gc | ⭐⭐ |
| 13 | 타임존 오류 | UTC 기본값 | Asia/Seoul 설정 | ⭐ |
| 14 | 정적 파일 404 | collectstatic 미실행 | whitenoise | ⭐ |

---

> **개발 회고**: 이 프로젝트를 통해 단순히 "동작하는 코드"가 아닌, 실제 운영 환경에서 발생할 수 있는 다양한 문제들을 경험했습니다. 특히 외부 API 의존성, 좌표 변환, 데이터 정합성 문제는 사전에 예측하기 어려웠지만, 체계적인 디버깅과 테스트를 통해 해결할 수 있었습니다.
